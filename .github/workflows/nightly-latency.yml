name: Nightly Latency Benchmark

on:
  schedule:
    - cron: '15 2 * * *'   # 02:15 UTC nightly
  workflow_dispatch:

permissions:
  contents: read

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    strategy:
      fail-fast: false
      matrix:
        scenario: [ small, medium, large ]
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
      - name: Install
        run: |
          pip install -U pip
          pip install -r requirements.txt || true
          pip install pytest pytest-benchmark rich
      - name: Run benchmark
        run: |
          pytest tests/benchmarks -k ${{ matrix.scenario }} --benchmark-json=bench.json
      - name: Archive result
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-${{ matrix.scenario }}
          path: bench.json

  aggregate:
    needs: benchmark
    runs-on: ubuntu-latest
    steps:
      - name: Download artifacts
        uses: actions/download-artifact@v4
        with:
          path: results
      - name: Aggregate
        run: |
          python - <<'PY'
          import json,glob,statistics,os
          latencies=[]
          for f in glob.glob('results/**/bench.json', recursive=True):
              data=json.load(open(f))
              for bench in data['benchmarks']:
                  latencies.append(bench['stats']['mean'])
          print('MEAN_LATENCY', sum(latencies)/len(latencies))
          PY
      - name: Summary
        run: echo "Nightly latency benchmarks completed" >> $GITHUB_STEP_SUMMARY