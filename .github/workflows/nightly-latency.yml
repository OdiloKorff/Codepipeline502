name: Nightly Latency Benchmark

on:
  schedule:
    - cron: '15 2 * * *'   # 02:15 UTC nightly
  workflow_dispatch:

permissions:
  contents: read

concurrency:
  group: nightly-latency
  cancel-in-progress: false

jobs:
  benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      MEAN_LATENCY_SLO: "0"   # "0" = deaktiviert
      P90_LATENCY_SLO: "0"
    defaults:
      run:
        shell: bash -euo pipefail
    steps:
      - name: Checkout
        uses: actions/checkout@v4
        with:
          fetch-depth: 0

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'
          cache: 'pip'
          cache-dependency-path: |
            requirements.txt
            pyproject.toml

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          if [[ -f requirements.txt ]]; then pip install -r requirements.txt; fi
          pip install pytest pytest-benchmark

      - name: Show dependency versions
        run: |
          python -V
          pip list

      - name: Run latency benchmarks
        run: |
          mkdir -p results
          if compgen -G "tests/benchmark/*.py" > /dev/null; then
            pytest tests/benchmark --benchmark-json=results/bench.json --maxfail=1 -q
          else
            pytest -k latency --benchmark-json=results/bench.json --maxfail=1 -q || echo "No specific latency tests matched; continuing."
          fi
          ls -R results || true

      - name: Upload raw benchmark artifact
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: nightly-latency-raw
          path: results
          retention-days: 14
          if-no-files-found: warn

      - name: Aggregate metrics
        id: aggregate
        run: |
          python - <<'PY'
          import json, glob, statistics, os, math
          files = glob.glob("results/**/bench.json", recursive=True)
          latencies=[]
          for f in files:
              try:
                  data=json.load(open(f))
              except Exception as e:
                  print(f"::warning file={f}::{e}")
                  continue
              for bench in data.get("benchmarks", []):
                  mean = bench.get("stats", {}).get("mean")
                  if isinstance(mean,(int,float)) and not math.isnan(mean):
                      latencies.append(mean)
          out_path = os.environ.get("GITHUB_OUTPUT")
          if not latencies:
              print("::warning ::No latency samples collected.")
              with open(out_path,"a") as gh:
                  for k in ("mean","best","worst","p90"):
                      gh.write(f"{k}=\n")
                  gh.write("count=0\n")
          else:
              latencies.sort()
              mean = statistics.fmean(latencies)
              best = latencies[0]
              worst = latencies[-1]
              idx = int(round(0.90*(len(latencies)-1)))
              p90 = latencies[idx]
              with open(out_path,"a") as gh:
                  gh.write(f"mean={mean}\n")
                  gh.write(f"best={best}\n")
                  gh.write(f"worst={worst}\n")
                  gh.write(f"p90={p90}\n")
                  gh.write(f"count={len(latencies)}\n")
              print(f"Collected {len(latencies)} samples")
              print(f"Mean:  {mean:.6f}s")
              print(f"P90:   {p90:.6f}s")
              print(f"Best:  {best:.6f}s")
              print(f"Worst: {worst:.6f}s")
          PY

      - name: Enforce latency SLOs
        if: steps.aggregate.outputs.count != '0'
        run: |
          fail=0
          mean="${{ steps.aggregate.outputs.mean }}"
            p90="${{ steps.aggregate.outputs.p90 }}"
          if [[ "${MEAN_LATENCY_SLO}" != "0" && $(echo "${mean} > ${MEAN_LATENCY_SLO}" | bc -l) -eq 1 ]]; then
            echo "::error ::Mean latency ${mean}s exceeds SLO ${MEAN_LATENCY_SLO}s"
            fail=1
          fi
          if [[ "${P90_LATENCY_SLO}" != "0" && $(echo "${p90} > ${P90_LATENCY_SLO}" | bc -l) -eq 1 ]]; then
            echo "::error ::P90 latency ${p90}s exceeds SLO ${P90_LATENCY_SLO}s"
            fail=1
          fi
          if [[ $fail -eq 1 ]]; then
            exit 1
          fi

      - name: Append summary
        run: |
          {
            echo "## Nightly Latency Benchmark"
            echo ""
            if [[ "${{ steps.aggregate.outputs.count }}" == "0" ]]; then
              echo "> ⚠️ Keine Benchmark-Daten gefunden."
            else
              echo "| Metric | Seconds |"
              echo "|--------|---------|"
              echo "| Mean   | ${{ steps.aggregate.outputs.mean }} |"
              echo "| P90    | ${{ steps.aggregate.outputs.p90 }} |"
              echo "| Best   | ${{ steps.aggregate.outputs.best }} |"
              echo "| Worst  | ${{ steps.aggregate.outputs.worst }} |"
              echo ""
              echo "Samples: ${{ steps.aggregate.outputs.count }}"
            fi
          } >> "$GITHUB_STEP_SUMMARY"

